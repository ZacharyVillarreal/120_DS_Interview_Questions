{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis - 27 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (Given a Dataset) Analyze this dataset and tell me what you can learn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * can't answer this question w.out a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is $R^2$? What are some other metrics that could be better than R2 and why?\n",
    "    *  goodness of fit measure\n",
    "    *  proportion of the variance in the dependent variable that is predictable from the independent variable\n",
    "    *  the more predictors you add, the higher $R^2$ becomes\n",
    "        *  Use the adjusted $R^2$, which adjusts for the degrees of freedom\n",
    "        *  train error metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the curse of dimensionality?\n",
    "    *  High dimensionality makes clustering difficult, b/c having lots of dimensions (predictors) means that everything is \"further away\" from eachother\n",
    "    *  Ex.\n",
    "        *  To cover a fraction of the volume of data we need to capture a very wide range for each variable as the number of variables increase\n",
    "        *  All samples are close to the edge of the sample, this is bad news b/c prediction is much more difficult near the edges of the training sample\n",
    "        *  The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Is more data always better?\n",
    "    *  Statistically:\n",
    "        * It depends on the quality of the data, if you're data is incorrect or biased, more of the same data won't help, it'll definitely hurt.\n",
    "        * It also depends on the model the Data Scientist is running. If the model is prone to suffering from high bias or high dimensionality, getting more data won't improve your model's results.\n",
    "    * Practically:\n",
    "        * There's also a tradeoff when it comes to having more data, like computing power, storage, time it may require. So more data isn't always better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are the advantages of plotting your data before performing data analysis?\n",
    "    * Data sets are often riddled with errors. You won't be able to find all of them by running simples queries, sometimes a visual representation of your data can allow the Data Scientist to find it more easily.\n",
    "    * Variables can have skewness, outliers, if this is the case then the computing the mean might not be as useful. Which means the standard deviation isn't useful.\n",
    "    * Variables could also be multimodal. You could also determine if things are correlated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How can you make sure that you don't analyze something that ends up meaningless?\n",
    "    * Proper Exploratory Data Analysis(EDA)\n",
    "    * In every data analyst task, there's the exploratory phase where you're just graphing things, testing thigns on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further.\n",
    "    * Then there's the exploratory phase where you look deepling into a set of hypothesis\n",
    "    * The exploratory phase will generate lots of possible hypotheses, and the exploratory pahse will let you really understand a few of them.\n",
    "    * Balance of the two, you'll prevent yourself from wasting time on many things that end up meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is the role of trial and error in data analysis? What is the role of making a hypothesis before diving in?\n",
    "    * Data Analysis is a repitition of setting up new hypotheses generated from your EDA and trying to refute the null hypothesis.\n",
    "    * The scientific method is eminently inductive: \n",
    "        * we elaborate a hypothesis, test it and refute the null or not.\n",
    "        * As a result, we come up with new hypotheses which are in turn tested and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How can you determine which features are the most important in your model?\n",
    "    * For linear regression you can use p-value\n",
    "    * Run the features through a Gradient Boosting model or Random Forest and calculate the plots of relative importance and information gain for each feature in the ensemble\n",
    "    * Look at the variables added in forward variable selection\n",
    "    * Can also use a heatmap to look at how the features are related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How do you deal with some of your predictors being missing?\n",
    "    * You can remove the rows with missing values (THIS CAN BE VERY DETRIMENTAL, ONLY DO UNDER SPECIFIC CIRCUMSTANCES)\n",
    "        * If the values are missing randomly\n",
    "        * If you don't lose too much of the dataset after doing so\n",
    "    * Build another predictive model to predict those missing values\n",
    "        * This could become very cumbersome quickly, only use simple techniques are used.\n",
    "    * Use a model that incorporate missing data\n",
    "        * Like a random forest, or any tree-based method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?\n",
    "    * Multicollinearity referes to a situation in which two or more explanatory variables(predictors) in a multiple regression model that are highly linearly related.\n",
    "    * Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variabels follow the same pattern of multicollinearity in the new data as in the data on which the model is trained.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Let’s say you’re given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?\n",
    "    * PCA - principal component analysis: which is used to reduce the dimensionality of a data set:\n",
    "        * PCs (principal components) are essentially linear combinations of the original variables, the wieghts vector in this combination is actually the eigenvector found which in turn satifies the principle of least squares\n",
    "        * PCs are orthogonal\n",
    "        * The variation present in the PCs decrease as we move from the 1st PC to the last one, hence the imporance.\n",
    "    * PCA steps:\n",
    "        * Normalize the data\n",
    "        * Calculate the covariance matrix\n",
    "        * Calculate the eigenvalues and eigenvalues\n",
    "        * Choosing Components and forming a feature vector\n",
    "        * Form Principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Now you have a feasible amount of predictors, but you’re fairly sure that you don’t need all of them. How would you perform feature selection on the dataset?\n",
    "    * Ridge / Lasso\n",
    "    * Univariate Feature Selection: where a statistical test is applied to each feature individually. You only retain the best features according to the test outcome scores\n",
    "    * Recursive Feature Elimination:\n",
    "        * First, train a model with all the features and evaluate its performance on held out data\n",
    "        * Then, drop let's say 10% of the weakest features (i.e. the features with the least absolute coefficients in a linear model) and retrain on the remaining features\n",
    "        * Iterate until you observe a sharp drop in the predictive accuracy of the model.\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?\n",
    "    * If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?\n",
    "    * The dataset is heterogenous. \n",
    "    * To overcome this, the dataset should be clustered into different subsets and then separate models should be built for each cluster.\n",
    "    * Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogenuous data efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?\n",
    "    * The assumption behind ensemble learning is that a group of weak learners can be combined to form a strong leaner\n",
    "    * Hence the combined model is expected to perform better than an individual model\n",
    "    * Assumptions:\n",
    "        * Average out biases\n",
    "        * Reduce variance\n",
    "    * Bagging works b/c some underlying algorithms are unstable: slightly different inputs leags to very different outputs. If you can advantage of this instability by running multiple instances, it can be shown that the reduced instability leads to lower error.\n",
    "        * When bootstrap samples are correlated, the benefit of bagging decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Given that you have wifi data in your office, how would you determine whic rooms and areas are underutilized and overutilized. \n",
    "    * If there are more data entries coming from one room than others, than that room is over utilized. Maybe account for the room capacity and normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. What is the use of regularisation? Explain L1 and L2 regularisations.\n",
    "    * L1 or LASSo regularization:\n",
    "        * The absolute values of the coefficients are added to the cost function. (leads to feature selection)\n",
    "    * L2 or RIDGE regularization:\n",
    "        * The squares of the coefficients are added to the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. How could you use GPS data from a car to determine the quality of a driver?\n",
    "    * You could look at specific data patterns, or their driving habits.\n",
    "    * What is their speed in specific zones, are they taking sharp corners or constantly braking\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Given accelerometer, altitude, and fuel usage data from a car, how would you determine the optimum acceleration pattern to drive over hills?\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Given position data of NBA players in a season’s games, how would you evaluate a basketball player’s defensive ability?\n",
    "    * Figure out the position that the specific player plays, then identify the offensive player in that same position, to figure out who they are guarding. Then take the average, or the median (depending on distribution of points), and compare it to the league average of those in the same position, and overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. How would you quantify the influence of a Twitter user?\n",
    "    * This is a difficult qestion to answer, as \"influence\" is sort of a vague term. Simply looking at the amount of follower's a user has does not imply that they are in fact an \"influencer.\" I would say it's a combination of looking at the subject matter of their tweets, plus followers, retweets, likes, comments, things of that nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Given location data of golf balls in games, how would construct a model that can advise golfers where to aim?\n",
    "    * If you wanted a very \"high level\" model, you could just simply look at position of where each ball lands, and which position of ball gives you the least amount of strokes. In a more advanced model, you could look at weather patterns, type of club, type of ball, quality of the course, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of difficulty?\n",
    "    * One way to solve the problem is that you can store a \"skill level\" for each user and a \"difficulty level\" for each problem. We then assume that the probability that a user solves a problem only depends on the skill of the user and the difficulty of the problem. Then we maximize the likelihoodof the data to find the hidden skill and difficulty levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?\n",
    "    * I would probably takethe mean rank of each sushi, or I could simply use the median, since the ranks are ordinal and non-interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?\n",
    "    * We can use collaborative filtering, i.e. recommender systems, we then use your votes and can cacluate the similarity for each representatives and select the most similar representative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. How would you come up with an algorithm to detect plagiarism in online content?\n",
    "    * First clean the text, and then reduce it to a smaller amount of words, i.e. lemmatizing, and create a bag of words, then compare those with other texts by calculating the similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. You have data on all purchases of customers at a grocery store. Describe to me how you would program an algorithm that would cluster the customers into groups. How would you determine the appropriate number of clusters to include?\n",
    "    * You could use k-means\n",
    "    * Choose a small value for k that still has a low SSE(elbow plot), visualize these clusters and then determine what the best number of clusters for your data is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. Let’s say you’re building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?\n",
    "    * Content-Based filtering\n",
    "    * Collaborative filtering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda7d173c5ac3d647bb86950a56c57e9e53"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
